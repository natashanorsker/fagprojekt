{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "# Detectron2 training notebook\n",
    "Based on the [official Detectron 2 tutorial](\n",
    "https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)\n",
    "\n",
    "This is a notebook that can generate the model used for the image segmentation. It is intended to be used on google colab, but can easily be changed to run locally.\n",
    "\n",
    "To run this notebook you need some extra files which are available in our google drive folder detectron2segment. The train and val folders, they contain images and their respective polygon annotations.\n",
    "\n",
    "The output you need to save from this notebook is\n",
    "- model_final.pth\n",
    "- config.yml\n",
    "\n",
    "These are used to run inference and should be placed in this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_FzH13EjseR",
    "outputId": "41809c15-cae9-44aa-b66c-d749313de8dc"
   },
   "outputs": [],
   "source": [
    "# install dependencies: \n",
    "!pip install pyyaml==5.1\n",
    "!gcc --version\n",
    "\n",
    "# CUDA 10.2\n",
    "!pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\n",
    "# opencv is pre-installed on colab\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYIhu1E0cyzf",
    "outputId": "fe4f5628-ad66-4763-9543-2cd6776205f3"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b-i4hmGYk1dL",
    "outputId": "16164fe9-7caf-4e56-abf9-efa2582e3fee"
   },
   "outputs": [],
   "source": [
    "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "import torch\n",
    "assert torch.__version__.startswith(\"1.7\")   # please manually install torch 1.7 if Colab changes its default version\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n",
    "exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyAvNCJMmvFF",
    "outputId": "e6304de9-3bd1-422a-cc04-e66acafd205c"
   },
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "# Run a pre-trained detectron2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgKyUL4pngvE"
   },
   "source": [
    "We first download an image from the COCO dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "dq9GY37ml1kr",
    "outputId": "9edbe5e8-131c-41c2-83fd-b337f1d325d4"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "im = cv2.imread(\"./input.jpg\")\n",
    "#im = cv2.imread(\"./drive/MyDrive/data/train/brace.jpg\")\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUjkwRsOn1O0",
    "outputId": "0ba67f00-5810-4a3f-a7a5-9cb9c947c5c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "L-80pQRu6-xJ",
    "outputId": "b6af1b96-dbd1-4f65-fa23-93eab3d1b1d5"
   },
   "outputs": [],
   "source": [
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d3KxiHO_0gb",
    "outputId": "341b16c9-85b1-431c-ebfa-6408db46c015"
   },
   "outputs": [],
   "source": [
    "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "print(outputs[\"instances\"].pred_classes)\n",
    "print(outputs[\"instances\"].pred_boxes)\n",
    "print(outputs[\"instances\"].pred_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq0GBJ8XdBgV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf1lp2C1dSCy"
   },
   "outputs": [],
   "source": [
    "outputs[\"instances\"].pred_masks[0].shape\n",
    "im = cv2.imread(\"./input.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "kGsIS_1Qc78H",
    "outputId": "b7a7d26a-bd4c-4199-d81c-d19502886443"
   },
   "outputs": [],
   "source": [
    "plt.imshow(outputs[\"instances\"].pred_masks[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "G46uzAtHfQna",
    "outputId": "a41c5b7f-d239-4743-d414-4eebd0b06997"
   },
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvNKVOatxHVx"
   },
   "source": [
    "Extract relevant element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "oMrYABZSs7p-",
    "outputId": "d74af191-b08a-4b43-eddc-31f970d2b837"
   },
   "outputs": [],
   "source": [
    "def extract(im):\n",
    "    mask2 = np.asarray(outputs[\"instances\"].pred_masks[0].cpu())*1\n",
    "    mask3d = np.dstack((np.dstack((mask2, mask2)),mask2))*1\n",
    "    # mask by multiplication, clip to range 0 to 255 and make integer\n",
    "    result2 = (im * mask3d).clip(0, 255).astype(np.uint8)\n",
    "    result2[mask3d==0] = 255\n",
    "    box = np.asarray(outputs[\"instances\"].pred_boxes[0].to('cpu').tensor[0],dtype=int)\n",
    "    crop_img = result2[box[1]:box[3], box[0]:box[2]]\n",
    "    return crop_img\n",
    "\n",
    "crop_img = extract(im)\n",
    "cv2_imshow(crop_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXMh_uht4hVp"
   },
   "source": [
    "prøv den på et smykke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2bjrfb2LDeo"
   },
   "source": [
    "# Train on a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIbAM2pv-urF"
   },
   "outputs": [],
   "source": [
    "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
    "# from detectron2.data.datasets import register_coco_instances\n",
    "# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n",
    "# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtXITAFHQlxR"
   },
   "source": [
    "På smykkerne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ULumCaRQlIw",
    "outputId": "976c6b64-d3be-41c3-ed69-eea909e56648"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2flp40vBehUr"
   },
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "def get_jewellery_dicts(directory):\n",
    "    \"\"\"\n",
    "    NB: the image labels used to train must have more than 6 vertices.\n",
    "    \"\"\"\n",
    "    classes = ['Jewellery']\n",
    "    dataset_dicts = []\n",
    "    i = 0\n",
    "    for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\n",
    "        json_file = os.path.join(directory, filename)\n",
    "        with open(json_file) as f:\n",
    "            img_anns = json.load(f)\n",
    "\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
    "\n",
    "        record[\"image_id\"] = i\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"height\"] = 340\n",
    "        record[\"width\"] = 510\n",
    "\n",
    "        i+=1     \n",
    "\n",
    "\n",
    "        annos = img_anns[\"shapes\"]\n",
    "        objs = []\n",
    "        for anno in annos:\n",
    "            px = [a[0] for a in anno['points']]\n",
    "            py = [a[1] for a in anno['points']]\n",
    "            poly = [(x, y) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": classes.index(anno['label']),\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(\"Jewellery_\" + d, lambda d=d: get_jewellery_dicts('/content/drive/MyDrive/detectron2segment/' + d))\n",
    "    MetadataCatalog.get(\"Jewellery_\" + d).set(thing_classes=['Jewellery'])\n",
    "jewellery_metadata = MetadataCatalog.get(\"Jewellery_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "qq2urOo5Qkpj",
    "outputId": "ba501b90-274b-482f-ca16-522d795484b6"
   },
   "outputs": [],
   "source": [
    "dataset_dicts = get_jewellery_dicts(\"drive/MyDrive/detectron2segment/train\")\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=jewellery_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpeJSbEonouA"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"Jewellery_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 1000    # 1000 iterations seems good enough for this  dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (jewellery). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRPtJ3lyQiJr"
   },
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "f = open('config.yml', 'w')\n",
    "f.write(cfg.dump())\n",
    "f.close()\n",
    "\n",
    "# cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/detectron2segment/model_final.pth\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.82   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "s8CG0gEZlgQ-",
    "outputId": "df96cabd-41cb-47ad-81d5-2196ff749673"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "dataset_dicts = get_jewellery_dicts(\"drive/MyDrive/detectron2segment/val\")\n",
    "for d in random.sample(dataset_dicts, 5):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1], \n",
    "                   scale=0.5   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "JJ-wEwoafsKc",
    "outputId": "f2c4ca9d-12a9-4267-dc25-1925d69d853c"
   },
   "outputs": [],
   "source": [
    "im = cv2.imread(\"test.jpg\")\n",
    "outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "v = Visualizer(im[:, :, ::-1], \n",
    "                scale=0.5  # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    ")\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "FyAe_e6Xlf-Y",
    "outputId": "d551f2b2-77e7-4699-9c22-3209327db322"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"Jewellery_val\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"Jewellery_val\")\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Detectron2 on jewellery.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "c94be41889154d41bf43aca8d1a8d1cd64b97c119170e03e2ed46ca87183f0c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}